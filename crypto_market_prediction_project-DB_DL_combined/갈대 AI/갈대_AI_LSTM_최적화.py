# -*- coding: utf-8 -*-
"""1-1. wikidocs.net_44249 최적화.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fUXojAi1t-qHhiiBA3DSJZj_C5j2kcn6
"""

from tensorflow.python.client import device_lib
device_lib.list_local_devices()
import tensorflow as tf
tf.test.is_gpu_available()
tf.__version__
#conda activate directml
tf.debugging.set_log_device_placement(True)











!pip install pandas
!pip install matplotlib
!pip install konlpy
!pip install tqdm
!pip install tweepy
!pip install sklearn
!pip install keras
!pip install tweepy==3.10.0
#!pip install tensorflow

#%%

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.python.client import device_lib
#%%
"""## 데이터 불러오기"""

"C:\My_data\cryptocurrency_sentiment\crypto_labeling_Korean\eth.xlsx"

alt = pd.read_excel("C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/alt.xlsx")
btc = pd.read_excel("C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/btc.xlsx")
eth = pd.read_excel("C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/eth.xlsx")
xrp = pd.read_excel("C:/My_data//ryptocurrency_sentiment/crypto_labeling_Korean/xrp.xlsx")

"""## 데이터 합치기"""

all = pd.concat([alt, btc, eth, xrp])
all.info()

"""* 열 삭제"""

all.drop(['Unnamed: 4'], axis=1, inplace=True)
all.columns

"""* 코인과 상관없는 글 (3) 삭제
* 0 긍정, 1 중립, 2 부정
"""

# 한번 생각해보기!
all = all[all['labeling'] != 3]
all['labeling'].value_counts()

all.info()

"""* 제목과 내용 합치기"""

all = all.replace(np.nan, "")

all['title'] = all['title'].astype(str)
all['body'] = all['body'].astype(str)

all = all.replace(np.nan, "")
all['text'] = all['title']+ " " + all['body']

all.head()

all.drop(['title', 'body'], axis=1, inplace=True)
all.columns

"""## train_test_split"""

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(all, test_size = 0.2,
                                         random_state = 2045,
                                         stratify = all['labeling'])

"""## 데이터 정제하기"""

train_data.info()

train_data['text'].nunique(), train_data['labeling'].nunique()

train_data.drop_duplicates(subset=['text'], inplace=True)

print('총 샘플의 수 :', len(train_data))

train_data['labeling'].value_counts().plot(kind='bar')

print(train_data.groupby('labeling').size().reset_index(name='count'))

print(train_data.isnull().values.any())

"""## 한글 외 문자 제거"""

train_data['text'] = train_data['text'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '')
train_data.head()

train_data['text'] = train_data['text'].str.replace('^ +', '')
train_data['text'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

train_data = train_data.dropna(how = 'any')
print(len(train_data))

test_data.drop_duplicates(subset = ['text'], inplace = True)
test_data['text'] = test_data['text'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]', '')
test_data['text'] = test_data['text'].str.replace('^ +', '')
test_data['text'].replace('', np.nan, inplace=True)
test_data = test_data.dropna(how='any')
print('전처리 후 테스트용 샘플의 개수 :', len(test_data))

"""## 토큰화"""

stopwords = ['의', '가', '은', '들', '는', '좀', '잘', '걍',\
             '과', '도', '를', '으로', '자', '에', '와', '한', '하다']

okt = Okt()

X_train = []
for sentence in tqdm(train_data['text']):
    tokenized_sentence = okt.morphs(sentence, stem=True)
    stopwords_removed_sentence = [word for word in tokenized_sentence \
                                  if not word in stopwords]
    X_train.append(stopwords_removed_sentence)

print(X_train[:3])

X_test = []
for sentence in tqdm(test_data['text']):
    tokenized_sentence = okt.morphs(sentence, stem=True)
    stopwords_removed_sentence = [word for word in tokenized_sentence \
                                  if not word in stopwords]
    X_test.append(stopwords_removed_sentence)

"""## 정수 인코딩"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

print(tokenizer.word_index)

threshold = 3
total_cnt = len(tokenizer.word_index) # 단어의 수
rare_cnt = 0  # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0  # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if (value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 : ', total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수 : %s'%(threshold -1, rare_cnt))
print('단어 집합에서 희귀 단어의 비율 : ', (rare_cnt / total_cnt)*100)
print('전체 등장 빈도에서 희귀 단어 등장 빈도 비율 : ', (rare_freq / total_freq)*100)

# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.
# 0번 패딩 토큰을 고려하여 + 1
vocab_size = total_cnt - rare_cnt + 1
print('단어 집합의 크기 : ', vocab_size)

tokenizer = Tokenizer(vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

print(X_train[:3])

y_train = np.array(train_data['labeling'])
y_test = np.array(test_data['labeling'])

# 긍정,중립,부정 3분류 3_class
word_index = tokenizer.word_index

import json
json = json.dumps(word_index)
f3 = open('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/korIndex_3_class.json', 'w')
f3.write(json)
f3.close()

# # 긍정,중립,부정,배제 4분류 4_class
# word_index = tokenizer.word_index

# import json
# json = json.dumps(word_index)
# f3 = open('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/korIndex_4_class.json', 'w')
# f3.write(json)
# f3.close()

"""## 빈 샘플 (empty samples) 제거"""

drop_train = [index for index, sentence in enumerate(X_train) \
              if len(sentence) < 1]

X_train = np.delete(X_train, drop_train, axis = 0)
y_train = np.delete(y_train, drop_train, axis = 0)
print(len(X_train))
print(len(y_train))

"""## 패딩"""

print('리뷰의 최대 길이 : ', max(len(l) for l in X_train))
print('리뷰의 평균 길이 : ', sum(map(len, X_train))/len(X_train))
plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
def below_threshold_len(max_len, nested_list):
    count = 0
    for sentence in nested_list:
        if(len(sentence) <= max_len):
            count = count + 1
    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율 : %s'\
#           %(max_len, (count / len(nested_list))*100))

max_len = 100
below_threshold_len(max_len, X_train)

X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

type(X_train)

## 4진분류
# np.save('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/X_train.npy', X_train)
# np.save('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/X_test.npy', X_test)
# np.save('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/y_train.npy', y_train)
# np.save('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/y_test.npy', y_test)

# 3진분류
np.save('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/X_train_3_class.npy', X_train)
np.save('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/X_test_3_class.npy', X_test)
np.save('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/y_train_3_class.npy', y_train)
np.save('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/y_test_3_class.npy', y_test)
#%%
"""## ndarray 불러오기"""

# # 4진분류
# X_train = np.load('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/X_train.npy')
# X_test = np.load('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/X_test.npy')
# y_train = np.load('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/y_train.npy')
# y_test = np.load('/content/drive/MyDrive/Colab Notebooks/datasets/cryptocurrency_sentiment/crypto_labeling_Korean/y_test.npy')

# 3진분류
X_train = np.load('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/X_train_3_class.npy')
X_test = np.load('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/X_test_3_class.npy')
y_train = np.load('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/y_train_3_class.npy')
y_test = np.load('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/y_test_3_class.npy')

y_train

print(np.concatenate((y_train, y_test), axis=0))

X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)
#%%
"""## LSTM 감성 분류"""

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

#%%
#tf.debugging.set_log_device_placement(True)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
  except RuntimeError as e:
    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다
    print(e)
with tf.device('/CPU'):
    # vocab_size = 8430
    vocab_size = 6638
    
    embedding_dim = [25, 50, 100, 200, 400]
    hidden_units = [16, 32, 64, 128, 256, 512]
    epochs = [400]
    batch_size = [8, 16, 32, 64, 128]
    validation_split = [0.1,0.2]
    
    recoding = []
    
    for i in embedding_dim :
      for j in hidden_units :
        model = Sequential()
        model.add(Embedding(vocab_size, i))
        model.add(LSTM(j))
        model.add(Dense(3, activation='softmax'))
    
        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)
        # mc = ModelCheckpoint('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/K_4_class_best_model_1210.h5',
        mc = ModelCheckpoint('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/K_3_class_best_model_1210.h5',
                         monitor='val_acc', mode='max', verbose=1, save_best_only=True)
        
        for a in epochs:
          for b in batch_size:
            for c in validation_split:
              model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])
              history = model.fit(X_train, y_train, epochs=a, callbacks=[es, mc],
                                              batch_size=b, validation_split=c)
              
              # loaded_model = load_model('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/K_4_class_best_model_1210.h5')
              loaded_model = load_model('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/K_3_class_best_model_1210.h5')
              
              print('\n테스트 정확도 : %.4f' % (loaded_model.evaluate(X_test, y_test)[1]))
              print('embedding_dim : ', i, 'hidden_units : ', j, 'epoch : ', a, 'batch_size : ', b, 'validation_split : ', c,'\n\n')
              y = '%.4f' % (loaded_model.evaluate(X_test, y_test)[1])
    
              x_embedding_dim = i
              x_hidden_units = j
              x_epochs = a
              x_batch_size = b
              x_validation_split = c
    
              
    
              X = [x_embedding_dim, x_hidden_units ,x_epochs , x_batch_size, x_validation_split,y]
              recoding.append(X)
    
    dfr = pd.DataFrame(recoding)
    dfr.columns = ['embedding_dim', 'hidden_units', 'epochs', 'batch_size',
                   'validation_split', 'accuracy']
    dfr.to_csv('C:/My_data/cryptocurrency_sentiment/crypto_labeling_Korean/best_model_1212.csv', index =False)

#%%
embedding_dim = 100
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(3, activation = 'softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/3rd project/history/best_model_scc.h5',
                     monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc],
                    batch_size=64, validation_split=0.2)

loaded_model = load_model('/content/drive/MyDrive/Colab Notebooks/3rd project/history/best_model_scc.h5')
print('\n 테스트 정확도 : %.4f' % (loaded_model.evaluate(X_test, y_test)[1]))

"""## 리뷰 예측"""

def sentiment_predict(input, stopwords, vocab_size, word_index, loaded_model):
    input = input.replace(np.nan, "")
    input['text'] = input['title'] + ' ' + input['body']
    input.drop(['title', 'body'], axis=1, inplace=True)
    input['text'] = input['text'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '')

    new_sentence = []
    for sentence in tqdm(input['text']):
        tokenized_sentence = okt.morphs(sentence, stem=True)
        stopwords_removed_sentence = [word for word in tokenized_sentence \
                                      if not word in stopwords]
        new_sentence.append(stopwords_removed_sentence)

    tokenizer = Tokenizer(vocab_size)
    tokenizer.word_index = word_index
    encoded = tokenizer.texts_to_sequences(new_sentence)
    pad_new = pad_sequences(encoded, maxlen = max_len)
    # input['sentiment'] = loaded_model.predict(pad_new)

    # input['s'] = 1
    # input.loc[input['sentiment'] < 0.67, 's'] = 0
    # input.loc[input['sentiment'] >= 1.34, 's'] = 2

input = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/3rd project/data/kor/alt_x.csv', sep=',', encoding='cp949')

import json
with open('/content/drive/MyDrive/Colab Notebooks/3rd project/korIndex.json') as json_file:
    word_index = json.load(json_file)

from keras.models import load_model
loaded_model = load_model('/content/drive/MyDrive/Colab Notebooks/3rd project/history/best_model_mse.h5')

sentiment_predict(input, stopwords, vocab_size, word_index, loaded_model)

input.head()
#%%
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # 텐서플로가 첫 번째 GPU만 사용하도록 제한
  try:
    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
  except RuntimeError as e:
    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다
    print(e)
    
#%%
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
  except RuntimeError as e:
    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다
    print(e)
#%%
from tensorflow.python.client import device_lib
device_lib.list_local_devices()

    
